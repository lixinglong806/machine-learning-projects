{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze and Preprocess the raw_sample Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the analysis, we conclude that the prediction target is whether an advertisement is clicked (**clk** vs. **nonclk**).  \n",
    "Among the available fields, only **ad placement (pid)** serves as a useful feature for predicting ad clicks.  \n",
    "\n",
    "We then divide the dataset into **training set (first 7 days)** and **test set (the 8th day)** based on the timestamp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# 配置pyspark和spark driver运行时 使用的python解释器\n",
    "JAVA_HOME = '/root/bigdata/jdk'\n",
    "PYSPARK_PYTHON = '/miniconda2/envs/py365/bin/python'\n",
    "# 当存在多个版本时，不指定很可能会导致出错\n",
    "os.environ['PYSPARK_PYTHON'] = PYSPARK_PYTHON\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = PYSPARK_PYTHON\n",
    "os.environ['JAVA_HOME'] = JAVA_HOME\n",
    "# 配置spark信息\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "SPARK_APP_NAME = 'preprocessingRawSample'\n",
    "SPARK_URL = 'spark://192.168.58.100:7077'\n",
    "\n",
    "conf = SparkConf()    # 创建spark config对象\n",
    "config = (\n",
    "    (\"spark.app.name\", SPARK_APP_NAME),    # 设置启动的spark的app名称，没有提供，将随机产生一个名称\n",
    "    (\"spark.executor.memory\", \"2g\"),    # 设置该app启动时占用的内存用量，默认1g\n",
    "    (\"spark.master\", SPARK_URL),    # spark master的地址\n",
    "    (\"spark.executor.cores\", \"2\"),    # 设置spark executor使用的CPU核心数\n",
    "    # 以下三项配置，可以控制执行器数量\n",
    "    # (\"spark.dynamicAllocation.enabled\", True),\n",
    "    # (\"spark.dynamicAllocation.initialExecutors\", 1),    # 1个执行器\n",
    "    # (\"spark.shuffle.service.enabled\", True)\n",
    "    # ('spark.sql.pivotMaxValues', '99999'),  # 当需要pivot DF，且值很多时，需要修改，默认是10000\n",
    ")\n",
    "# 查看更详细配置及说明：https://spark.apache.org/docs/latest/configuration.html\n",
    "\n",
    "conf.setAll(config)\n",
    "\n",
    "# 利用config对象，创建spark session\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+----------+-----------+------+---+\n",
      "|  user|time_stamp|adgroup_id|        pid|nonclk|clk|\n",
      "+------+----------+----------+-----------+------+---+\n",
      "|581738|1494137644|         1|430548_1007|     1|  0|\n",
      "|449818|1494638778|         3|430548_1007|     1|  0|\n",
      "|914836|1494650879|         4|430548_1007|     1|  0|\n",
      "|914836|1494651029|         5|430548_1007|     1|  0|\n",
      "|399907|1494302958|         8|430548_1007|     1|  0|\n",
      "|628137|1494524935|         9|430548_1007|     1|  0|\n",
      "|298139|1494462593|         9|430539_1007|     1|  0|\n",
      "|775475|1494561036|         9|430548_1007|     1|  0|\n",
      "|555266|1494307136|        11|430539_1007|     1|  0|\n",
      "|117840|1494036743|        11|430548_1007|     1|  0|\n",
      "|739815|1494115387|        11|430539_1007|     1|  0|\n",
      "|623911|1494625301|        11|430548_1007|     1|  0|\n",
      "|623911|1494451608|        11|430548_1007|     1|  0|\n",
      "|421590|1494034144|        11|430548_1007|     1|  0|\n",
      "|976358|1494156949|        13|430548_1007|     1|  0|\n",
      "|286630|1494218579|        13|430539_1007|     1|  0|\n",
      "|286630|1494289247|        13|430539_1007|     1|  0|\n",
      "|771431|1494153867|        13|430548_1007|     1|  0|\n",
      "|707120|1494220810|        13|430548_1007|     1|  0|\n",
      "|530454|1494293746|        13|430548_1007|     1|  0|\n",
      "+------+----------+----------+-----------+------+---+\n",
      "only showing top 20 rows\n",
      "\n",
      "root\n",
      " |-- user: string (nullable = true)\n",
      " |-- time_stamp: string (nullable = true)\n",
      " |-- adgroup_id: string (nullable = true)\n",
      " |-- pid: string (nullable = true)\n",
      " |-- nonclk: string (nullable = true)\n",
      " |-- clk: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv(\"/data/raw_sample.csv\", header = True)\n",
    "df.show()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analyze Field Types and Formats in the Dataset  \n",
    "1. Check for missing values  \n",
    "2. Check the data type of each column  \n",
    "3. Check the category distribution of each column  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "样本数据集总条目数： 26557961\n",
      "用户user总数： 1141729\n",
      "广告id adgroup_id总数： 846811\n",
      "广告展示位pid情况： [Row(pid='430548_1007', count=16472898), Row(pid='430539_1007', count=10085063)]\n",
      "广告点击数据情况clk： [Row(clk='0', count=25191905), Row(clk='1', count=1366056)]\n"
     ]
    }
   ],
   "source": [
    "print(\"样本数据集总条目数：\", df.count())\n",
    "# 约2600w\n",
    "print(\"用户user总数：\", df.groupBy(\"user\").count().count())\n",
    "# 约 114w，略多余日志数据中用户数\n",
    "print(\"广告id adgroup_id总数：\", df.groupBy(\"adgroup_id\").count().count())\n",
    "# 约85w\n",
    "print(\"广告展示位pid情况：\", df.groupBy(\"pid\").count().collect())\n",
    "# 只有两种广告展示位，占比约为六比四\n",
    "print(\"广告点击数据情况clk：\", df.groupBy(\"clk\").count().collect())\n",
    "# 点和不点比率约： 1:20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use `dataframe.withColumn` to Modify Column Data Types; Use `dataframe.withColumnRenamed` to Rename Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user: string (nullable = true)\n",
      " |-- time_stamp: string (nullable = true)\n",
      " |-- adgroup_id: string (nullable = true)\n",
      " |-- pid: string (nullable = true)\n",
      " |-- nonclk: string (nullable = true)\n",
      " |-- clk: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- userId: integer (nullable = true)\n",
      " |-- timestamp: long (nullable = true)\n",
      " |-- adgroupId: integer (nullable = true)\n",
      " |-- pid: string (nullable = true)\n",
      " |-- nonclk: integer (nullable = true)\n",
      " |-- clk: integer (nullable = true)\n",
      "\n",
      "+------+----------+---------+-----------+------+---+\n",
      "|userId| timestamp|adgroupId|        pid|nonclk|clk|\n",
      "+------+----------+---------+-----------+------+---+\n",
      "|581738|1494137644|        1|430548_1007|     1|  0|\n",
      "|449818|1494638778|        3|430548_1007|     1|  0|\n",
      "|914836|1494650879|        4|430548_1007|     1|  0|\n",
      "|914836|1494651029|        5|430548_1007|     1|  0|\n",
      "|399907|1494302958|        8|430548_1007|     1|  0|\n",
      "|628137|1494524935|        9|430548_1007|     1|  0|\n",
      "|298139|1494462593|        9|430539_1007|     1|  0|\n",
      "|775475|1494561036|        9|430548_1007|     1|  0|\n",
      "|555266|1494307136|       11|430539_1007|     1|  0|\n",
      "|117840|1494036743|       11|430548_1007|     1|  0|\n",
      "|739815|1494115387|       11|430539_1007|     1|  0|\n",
      "|623911|1494625301|       11|430548_1007|     1|  0|\n",
      "|623911|1494451608|       11|430548_1007|     1|  0|\n",
      "|421590|1494034144|       11|430548_1007|     1|  0|\n",
      "|976358|1494156949|       13|430548_1007|     1|  0|\n",
      "|286630|1494218579|       13|430539_1007|     1|  0|\n",
      "|286630|1494289247|       13|430539_1007|     1|  0|\n",
      "|771431|1494153867|       13|430548_1007|     1|  0|\n",
      "|707120|1494220810|       13|430548_1007|     1|  0|\n",
      "|530454|1494293746|       13|430548_1007|     1|  0|\n",
      "+------+----------+---------+-----------+------+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 更改表结构，转换为对应的数据类型\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, FloatType, LongType, StringType\n",
    "\n",
    "# 打印df结构信息\n",
    "df.printSchema()   \n",
    "# 更改df表结构：更改列类型和列名称\n",
    "raw_sample_df = df.\\\n",
    "    withColumn(\"user\", df.user.cast(IntegerType())).withColumnRenamed(\"user\", \"userId\").\\\n",
    "    withColumn(\"time_stamp\", df.time_stamp.cast(LongType())).withColumnRenamed(\"time_stamp\", \"timestamp\").\\\n",
    "    withColumn(\"adgroup_id\", df.adgroup_id.cast(IntegerType())).withColumnRenamed(\"adgroup_id\", \"adgroupId\").\\\n",
    "    withColumn(\"pid\", df.pid.cast(StringType())).\\\n",
    "    withColumn(\"nonclk\", df.nonclk.cast(IntegerType())).\\\n",
    "    withColumn(\"clk\", df.clk.cast(IntegerType()))\n",
    "raw_sample_df.printSchema()\n",
    "raw_sample_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection  \n",
    "\n",
    "Feature selection means choosing reliable features and removing redundant ones.  \n",
    "- For **search ads**, the matching degree between the query keywords and the ad is very important.  \n",
    "- For **display ads**, the historical performance of the ad itself is usually the most important feature.  \n",
    "\n",
    "Based on experience, in this dataset only the **ad placement (pid)** is relatively important, with a distribution ratio of about 6:4 across different values. Therefore, `pid` can be used as a key feature.  \n",
    "\n",
    "`nonclk` and `clk` serve as the target labels here and are **not** used as features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-Hot Encoding  \n",
    "\n",
    "One-hot encoding is a classic encoding method that uses N binary bits (0/1) to represent N possible states. Each state has its own dedicated bit, and at any time only one bit is active.  \n",
    "\n",
    "Suppose we have three feature groups: age, city, and device:  \n",
    "\n",
    "- [\"Male\", \"Female\"] → [0,1]  \n",
    "- [\"Beijing\", \"Shanghai\", \"Guangzhou\"] → [0,1,2]  \n",
    "- [\"Apple\", \"Xiaomi\", \"Huawei\", \"Microsoft\"] → [0,1,2,3]  \n",
    "\n",
    "**Traditional mapping (enumeration):**  \n",
    "Each feature group is assigned values starting from 0.  \n",
    "- [\"Male\", \"Shanghai\", \"Xiaomi\"] → [0,1,1]  \n",
    "- [\"Female\", \"Beijing\", \"Apple\"] → [1,0,0]  \n",
    "\n",
    "This encoding is not continuous and is essentially arbitrary, making it less suitable for classifiers.  \n",
    "\n",
    "**After one-hot encoding:**  \n",
    "The data becomes sparse but more suitable for classifiers:  \n",
    "- [\"Male\", \"Shanghai\", \"Xiaomi\"] → [1,0,0,1,0,0,1,0,0]  \n",
    "- [\"Female\", \"Beijing\", \"Apple\"] → [0,1,1,0,0,1,0,0,0]  \n",
    "\n",
    "**This approach preserves the diversity of features, but note:** if the data becomes too sparse (few samples and very high dimensions), performance may actually degrade.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One-Hot Encoding in Spark  \n",
    "\n",
    "Note: One-hot encoding can only be applied to string-type columns.  \n",
    "\n",
    "- [StringIndexer](https://spark.apache.org/docs/latest/api/python/pyspark.ml.html?highlight=stringindexer#pyspark.ml.feature.StringIndexer): Processes specified string columns, e.g., converting gender values \"Male\" and \"Female\" into 0 and 1.  \n",
    "- [OneHotEncoder](https://spark.apache.org/docs/latest/api/python/pyspark.ml.html?highlight=onehotencoder#pyspark.ml.feature.OneHotEncoder): Performs one-hot encoding on feature columns; usually used together with `StringIndexer`.  \n",
    "- [Pipeline](https://spark.apache.org/docs/latest/api/python/pyspark.ml.html?highlight=pipeline#pyspark.ml.Pipeline): Ensures sequential processing of data, passing the result of one step as the input to the next.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[userId: int, timestamp: bigint, adgroupId: int, pid: string, nonclk: int, clk: int, pid_feature: double, pid_value: vector]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''特征处理'''\n",
    "'''\n",
    "pid 资源位。该特征属于分类特征，只有两类取值，因此考虑进行热编码处理即可，分为是否在资源位1、是否在资源位2 两个特征\n",
    "'''\n",
    "from pyspark.ml.feature import OneHotEncoder\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# StringIndexer对指定字符串 列数据进行特征处理，如将性别数据“男”、“女”转化为0和1\n",
    "stringindexer = StringIndexer(inputCol='pid',outputCol='pid_feature')\n",
    "# OneHotEncoder对特征列数据，进行热编码，通常需结合StringIndexer一起使用\n",
    "# dropLast=False 两个pid特征，onehot编码是(2,[0],[1.0])，即[0,1]和[1,0]\n",
    "# dropLast=Ture 两个pid特征，onehot编码是(1,[0],[1.0])，即[1]和[0]\n",
    "encoder = OneHotEncoder(dropLast = False, inputCol='pid_feature', outputCol='pid_value')\n",
    "pipline = Pipeline(stages=[stringindexer,encoder])\n",
    "pipline_model = pipline.fit(raw_sample_df)\n",
    "new_df = pipline_model.transform(raw_sample_df)# 返回pid_value是稀疏向量\n",
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+---------+-----------+------+---+-----------+-------------+\n",
      "|userId| timestamp|adgroupId|        pid|nonclk|clk|pid_feature|    pid_value|\n",
      "+------+----------+---------+-----------+------+---+-----------+-------------+\n",
      "|581738|1494137644|        1|430548_1007|     1|  0|        0.0|(2,[0],[1.0])|\n",
      "|449818|1494638778|        3|430548_1007|     1|  0|        0.0|(2,[0],[1.0])|\n",
      "|914836|1494650879|        4|430548_1007|     1|  0|        0.0|(2,[0],[1.0])|\n",
      "|914836|1494651029|        5|430548_1007|     1|  0|        0.0|(2,[0],[1.0])|\n",
      "|399907|1494302958|        8|430548_1007|     1|  0|        0.0|(2,[0],[1.0])|\n",
      "|628137|1494524935|        9|430548_1007|     1|  0|        0.0|(2,[0],[1.0])|\n",
      "|298139|1494462593|        9|430539_1007|     1|  0|        1.0|(2,[1],[1.0])|\n",
      "|775475|1494561036|        9|430548_1007|     1|  0|        0.0|(2,[0],[1.0])|\n",
      "|555266|1494307136|       11|430539_1007|     1|  0|        1.0|(2,[1],[1.0])|\n",
      "|117840|1494036743|       11|430548_1007|     1|  0|        0.0|(2,[0],[1.0])|\n",
      "|739815|1494115387|       11|430539_1007|     1|  0|        1.0|(2,[1],[1.0])|\n",
      "|623911|1494625301|       11|430548_1007|     1|  0|        0.0|(2,[0],[1.0])|\n",
      "|623911|1494451608|       11|430548_1007|     1|  0|        0.0|(2,[0],[1.0])|\n",
      "|421590|1494034144|       11|430548_1007|     1|  0|        0.0|(2,[0],[1.0])|\n",
      "|976358|1494156949|       13|430548_1007|     1|  0|        0.0|(2,[0],[1.0])|\n",
      "|286630|1494218579|       13|430539_1007|     1|  0|        1.0|(2,[1],[1.0])|\n",
      "|286630|1494289247|       13|430539_1007|     1|  0|        1.0|(2,[1],[1.0])|\n",
      "|771431|1494153867|       13|430548_1007|     1|  0|        0.0|(2,[0],[1.0])|\n",
      "|707120|1494220810|       13|430548_1007|     1|  0|        0.0|(2,[0],[1.0])|\n",
      "|530454|1494293746|       13|430548_1007|     1|  0|        0.0|(2,[0],[1.0])|\n",
      "+------+----------+---------+-----------+------+---+-----------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Returned Field `pid_value` Is a Sparse Vector Type\n",
    "\n",
    "#### [pyspark.ml.linalg.SparseVector](https://spark.apache.org/docs/latest/api/python/pyspark.ml.html?highlight=sparse#pyspark.ml.linalg.SparseVector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''=====延申学习：spark中的稀疏向量=====\n",
    "from pyspark.ml.linalg import SparseVector\n",
    "print(SparseVector(4,[1,3],[3.0,4.0]))\n",
    "print(SparseVector(4,[2,3],[3.0,4.0]).toArray())\n",
    "print('*********')\n",
    "print(new_df.select('pid_value').first())\n",
    "print(new_df.select('pid_value').first().pid_value.toArray())\n",
    "'''\n",
    "'''\n",
    "(4,[1,3],[3.0,4.0])\n",
    "[0. 0. 3. 4.]\n",
    "*********\n",
    "Row(pid_value=SparseVector(2, {0: 1.0}))\n",
    "[1. 0.]\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split the Dataset into Training and Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+---------+-----------+------+---+-----------+-------------+\n",
      "|userId| timestamp|adgroupId|        pid|nonclk|clk|pid_feature|    pid_value|\n",
      "+------+----------+---------+-----------+------+---+-----------+-------------+\n",
      "|243671|1494691186|   600195|430548_1007|     1|  0|        0.0|(2,[0],[1.0])|\n",
      "|177002|1494691186|   593001|430548_1007|     1|  0|        0.0|(2,[0],[1.0])|\n",
      "|488527|1494691184|   687854|430548_1007|     1|  0|        0.0|(2,[0],[1.0])|\n",
      "| 17054|1494691184|   742741|430548_1007|     1|  0|        0.0|(2,[0],[1.0])|\n",
      "|488527|1494691184|   431082|430548_1007|     1|  0|        0.0|(2,[0],[1.0])|\n",
      "| 17054|1494691184|   756665|430548_1007|     1|  0|        0.0|(2,[0],[1.0])|\n",
      "|488527|1494691184|   494312|430548_1007|     1|  0|        0.0|(2,[0],[1.0])|\n",
      "|839493|1494691183|   582235|430548_1007|     1|  0|        0.0|(2,[0],[1.0])|\n",
      "|704223|1494691183|   624504|430539_1007|     1|  0|        1.0|(2,[1],[1.0])|\n",
      "|839493|1494691183|   561681|430548_1007|     1|  0|        0.0|(2,[0],[1.0])|\n",
      "|704223|1494691183|   675674|430539_1007|     1|  0|        1.0|(2,[1],[1.0])|\n",
      "|628998|1494691180|   618965|430548_1007|     1|  0|        0.0|(2,[0],[1.0])|\n",
      "|627200|1494691179|   782038|430548_1007|     1|  0|        0.0|(2,[0],[1.0])|\n",
      "|674444|1494691179|   588664|430548_1007|     1|  0|        0.0|(2,[0],[1.0])|\n",
      "|322244|1494691179|   820018|430548_1007|     1|  0|        0.0|(2,[0],[1.0])|\n",
      "|627200|1494691179|   817569|430548_1007|     1|  0|        0.0|(2,[0],[1.0])|\n",
      "|322244|1494691179|   735220|430548_1007|     1|  0|        0.0|(2,[0],[1.0])|\n",
      "|738335|1494691179|   451004|430539_1007|     1|  0|        1.0|(2,[1],[1.0])|\n",
      "|627200|1494691179|   420769|430548_1007|     1|  0|        0.0|(2,[0],[1.0])|\n",
      "|674444|1494691179|   427579|430548_1007|     1|  0|        0.0|(2,[0],[1.0])|\n",
      "+------+----------+---------+-----------+------+---+-----------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_df.sort(\"timestamp\", ascending=False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "该时间之前的数据为训练样本，该时间以后的数据为测试样本： 2017-05-12 23:59:46\n"
     ]
    }
   ],
   "source": [
    "# 本样本数据集共计8天数据\n",
    "# 前七天为训练数据、最后一天为测试数据\n",
    "\n",
    "from datetime import datetime\n",
    "datetime.fromtimestamp(1494691186)\n",
    "print(\"该时间之前的数据为训练样本，该时间以后的数据为测试样本：\", datetime.fromtimestamp(1494691186-24*60*60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练样本个数：\n",
      "23249291\n",
      "测试样本个数：\n",
      "3308670\n"
     ]
    }
   ],
   "source": [
    "# `where` is an alias for :func:`filter`.\n",
    "# 训练样本：\n",
    "train_sample = raw_sample_df.where(raw_sample_df.timestamp<=(1494691186-24*60*60))\n",
    "print(\"训练样本个数：\")\n",
    "print(train_sample.count())\n",
    "# 测试样本\n",
    "test_sample = raw_sample_df.filter(raw_sample_df.timestamp>(1494691186-24*60*60))\n",
    "print(\"测试样本个数：\")\n",
    "print(test_sample.count())\n",
    "\n",
    "# 注意：还需要加入广告基本特征和用户基本特征才能做程一份完整的样本数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "1. user_id: anonymized user ID\n",
    "2. adgroup_id: anonymized ad unit ID\n",
    "3. time_stamp: timestamp\n",
    "4. pid: ad placement ID\n",
    "5. noclk: 1 = not clicked; 0 = clicked\n",
    "6. clk: 0 = not clicked; 1 = clicked\n",
    "'''\n",
    "# Only the ad placement (pid) is useful for predicting whether an ad is clicked.\n",
    "# Note: To build a complete sample dataset, we still need to include \n",
    "#       ad basic features and user basic features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
